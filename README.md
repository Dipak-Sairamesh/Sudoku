We investigate the underlying strategies of sudoku through
the efficiency of deep reinforcement learning algorithms,
specifically through Monte Carlo Tree Search, different CNN
architectures in Deep Q-Learning (DQN) and off-policy
Actor-Critic (AC) for creation and solution of puzzles. We
also investigate the effects of a replay buffer type on
performance and propose a distinctive replay buffer that
seems to work the best. The presented models were trained
on a dataset comprising of 3 million 9x9 puzzles and solutions. 
